{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c644ba",
   "metadata": {},
   "source": [
    "# mis notas para entender que sucedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0a81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(documents_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b691a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(documents_raw[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b68b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "print(json.dumps(documents_raw, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ab958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw[0]['course']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw[0]['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e3386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_name_to_find = \"machine-learning-zoomcamp\"\n",
    "# O para el ejemplo de la imagen:\n",
    "# course_name_to_find = \"mlops-zoomcamp\" # si quieres llegar a la pregunta 'Is it going to be live? When?'\n",
    "\n",
    "# Paso 1: Encontrar el diccionario del curso usando un bucle for\n",
    "found_course_data = None\n",
    "for course_dict in documents_raw:\n",
    "    if course_dict[\"course\"] == course_name_to_find:\n",
    "        found_course_data = course_dict\n",
    "        break # Detener la iteraciÃ³n una vez que encontramos el curso\n",
    "\n",
    "# Paso 2: Acceder a la primera pregunta si el curso fue encontrado\n",
    "if found_course_data:\n",
    "    # Asegurarse de que el curso tiene documentos\n",
    "    if found_course_data[\"documents\"]:\n",
    "        first_question = found_course_data[\"documents\"][0][\"question\"]\n",
    "        print(f\"La primera pregunta para '{course_name_to_find}' es: {first_question}\")\n",
    "    else:\n",
    "        print(f\"El curso '{course_name_to_find}' no tiene documentos.\")\n",
    "else:\n",
    "    print(f\"El curso '{course_name_to_find}' no fue encontrado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd44faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    courses.append( course['course'])\n",
    "\n",
    "courses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71fe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_raw[0]['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d25a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ea65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58a43b",
   "metadata": {},
   "source": [
    "# Original Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff866d6",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d44557-bc0d-43b1-8e4f-1a1973bd554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152dc317-118b-46f3-bc2e-5dee4f66c4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa21b7-adbf-44e4-bf02-d38998a12f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(documents, columns=['course', 'section', 'question', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b460b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9ecbf7-13ce-4a4f-baaf-5ad989bdeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.course == 'data-engineering-zoomcamp'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f4da02-d93a-412f-8a99-3b5d6c23056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_example = [\n",
    "    \"January course details, register now\",\n",
    "    \"Course prerequisites listed in January catalog\",\n",
    "    \"Submit January course homework by end of month\",\n",
    "    \"Register for January course, no prerequisites\",\n",
    "    \"January course setup: Python and Google Cloud\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f479e",
   "metadata": {},
   "source": [
    "## Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb8ab1",
   "metadata": {},
   "source": [
    "### ðŸ§  CountVectorizer & Bag of Words (BoW)\n",
    "\n",
    "`CountVectorizer` from `sklearn.feature_extraction.text` transforms text into a matrix of token counts. It is a practical implementation of the **Bag of Words (BoW)** technique.\n",
    "\n",
    "#### ðŸ“¦ What is Bag of Words?\n",
    "\n",
    "BoW is a text representation method where each document is converted into a vector of word frequencies, ignoring grammar and word order but keeping word occurrence.\n",
    "\n",
    "- Each row represents a document.\n",
    "- Each column represents a unique word (token).\n",
    "- Values indicate how many times each word appears in that document.\n",
    "\n",
    "#### âš™ï¸ Example:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\"I love data\", \"I love AI\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# ['ai' 'data' 'love']\n",
    "\n",
    "print(X.toarray())\n",
    "# [[0 1 1]\n",
    "#  [1 0 1]]\n",
    "\n",
    "\n",
    "ðŸ§  Note:\n",
    "This technique does not capture the meaning or order of words, but itâ€™s simple and often effective for traditional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f62369-fc0d-484f-abc4-ba846a3a4bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51522f4-2e65-42ef-8ccc-310adb032658",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c5bdbc-aaaa-4da6-b19d-5f9d4fdb7eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(docs_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8041bcfe-0b1b-4c06-ba64-917440f89f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = cv.get_feature_names_out()\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151c0e0-bfa1-4ea4-9cbd-e3544ebfcb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.transform(docs_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bca2a-4af8-4ccf-82a5-310952a1cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af73f9-96e1-4e3b-b9a1-34fd9c60dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = pd.DataFrame(X.toarray(), columns=names).T\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9630a4d-797d-45d8-9849-2c4b3c02c249",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(docs_example)\n",
    "\n",
    "names = cv.get_feature_names_out()\n",
    "\n",
    "df_docs = pd.DataFrame(X.toarray(), columns=names).T\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d90a25",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e944b436",
   "metadata": {},
   "source": [
    "### ðŸ“Š TfidfVectorizer & TF-IDF\n",
    "\n",
    "`TfidfVectorizer` from `sklearn.feature_extraction.text` transforms text into a matrix of **TF-IDF scores** (Term Frequencyâ€“Inverse Document Frequency). It's a refinement of the **Bag of Words** model that reduces the impact of common words and emphasizes more informative ones.\n",
    "\n",
    "#### ðŸ§  What is TF-IDF?\n",
    "\n",
    "TF-IDF measures how important a word is to a document in a collection (corpus). It balances:\n",
    "- **Term Frequency (TF):** how often a word appears in a document.\n",
    "- **Inverse Document Frequency (IDF):** how rare the word is across all documents.\n",
    "\n",
    "**High TF + Low DF = High importance.**\n",
    "\n",
    "#### âš™ï¸ Example:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\"I love data\", \"I love AI\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# ['ai' 'data' 'love']\n",
    "\n",
    "print(X.toarray())\n",
    "# Might return values like:\n",
    "# [[0.0   0.707 0.707]\n",
    "#  [0.707 0.0   0.707]]\n",
    "````\n",
    "\n",
    "#### â„¹ï¸ Interpretation:\n",
    "\n",
    "* Values near **1** â†’ terms that are unique or highly relevant in that document.\n",
    "* Values near **0** â†’ common words across the corpus or not present.\n",
    "\n",
    "#### ðŸ§  Note:\n",
    "\n",
    "TF-IDF improves over simple count models by **reducing the weight of frequent but less informative words** (like \"the\", \"is\", etc.), often making models more accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b9f3d6-5d93-4a29-baf6-4cf8c5cf1d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a63806-bbb3-4238-ad78-11aed2c1c9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(stop_words='english')\n",
    "X = cv.fit_transform(docs_example)\n",
    "\n",
    "names = cv.get_feature_names_out()\n",
    "\n",
    "df_docs = pd.DataFrame(X.toarray(), columns=names).T\n",
    "df_docs.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34bdd31",
   "metadata": {},
   "source": [
    "\n",
    "> ### ðŸ” When to use `fit_transform()` vs `transform()`\n",
    "\n",
    "- **`fit_transform()`**  \n",
    "  Use this on your **training data**. It:\n",
    "  1. Learns the vocabulary or statistical parameters (fit).\n",
    "  2. Applies the transformation to the same data (transform).\n",
    "\n",
    "  âœ… Applies to the first dataset (e.g., training set).\n",
    "\n",
    "- **`transform()`**  \n",
    "  Use this on **new or test data**. It:\n",
    "  1. Applies the previously learned vocabulary or parameters.\n",
    "\n",
    "  âœ… Ensures consistency between train and test sets.\n",
    "\n",
    "\n",
    "#### âš ï¸ Example:\n",
    "\n",
    "```python\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_docs)  # Learn & transform\n",
    "X_test = vectorizer.transform(test_docs)        # Only transform\n",
    "````\n",
    "\n",
    "**Never call `fit()` or `fit_transform()` on test data**, or you'll cause data leakage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e218958",
   "metadata": {},
   "source": [
    "## Do the search with dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39d3be-281d-4d01-a645-e5a49678f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Do I need to know python to sign up for the January course?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0f5f35-b1b4-4e8a-b0af-7b83e8830b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = cv.transform([query])\n",
    "q.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb650c2-5ec3-4419-a27c-8db1af022206",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dict = dict(zip(names, q.toarray()[0]))\n",
    "query_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099d689-1156-4f49-8cfa-4bdda6c60734",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(names, X.toarray()[1]))\n",
    "doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4600a5-1924-4680-acc9-55b1f34a4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qd = pd.DataFrame([query_dict, doc_dict], index=['query', 'doc']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afb041c-da6f-4f20-b58e-12bc1800c223",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_qd['query'] * df_qd['doc']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9ebc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe38cf2-5925-48c1-be6b-4867e63b1731",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dot(q.T).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff3df5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dot(q.T).todense()\n",
    "# doesn't change because there are only 5 elements in the matrix, and everyone has similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0360266",
   "metadata": {},
   "source": [
    "ðŸ” Dot Product for Document Ranking\n",
    "\n",
    "In vector-based search (e.g., with TF-IDF), each document and query are represented as vectors. By computing the **dot product between the query vector and each document vector**, we obtain a relevance score for each document.\n",
    "\n",
    "- **Higher score â†’ more relevant document**\n",
    "- This method is efficient and works well with sparse vector representations.\n",
    "- If vectors are normalized, the dot product equals **cosine similarity**.\n",
    "\n",
    "The top results are obtained by sorting documents in descending order of their scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c49ec8-1ddb-4c77-bbd9-87afd6aaeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d49a2a-1e7e-432d-b5fe-cb7d83a7eef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(X, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c12f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(X, q).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e466b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cosine_similarity(X, q).flatten()\n",
    "np.argsort(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96103d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42f4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[4].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e77c501b449eb96",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a0cbe2543cd99",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields = ['section', 'question', 'text']\n",
    "transformers = {}\n",
    "matrices = {}\n",
    "\n",
    "for field in fields:\n",
    "    cv = TfidfVectorizer(stop_words='english', min_df=3)\n",
    "    X = cv.fit_transform(df[field])\n",
    "\n",
    "    transformers[field] = cv\n",
    "    matrices[field] = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442526e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers['text'].get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03aea39445ae9e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrices['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dd06e91817dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I just singned up. Is it too late to join the course?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e48c3a96d93e583",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = transformers['text'].transform([query])\n",
    "score = cosine_similarity(matrices['text'], q).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35c13cc-ae93-4b64-b6f2-8519f1b1175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (df.course == 'data-engineering-zoomcamp').values\n",
    "score = score * mask\n",
    "score[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af8de09-054e-4638-af4d-27eb9335187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd76b3b5-092c-4c4d-b06b-c1a2ea73e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(-score)[:10]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5e093-18a5-4e07-b1ba-825ae8281144",
   "metadata": {},
   "outputs": [],
   "source": [
    "score[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1629617-e109-4aeb-a758-42b621665bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[idx].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070e9bfc-7364-4746-8a13-5429c1e3793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a0f583-7c95-493d-b54e-19737c4e6807",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I just signed up. Is it too late to join the course?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde3950-b575-4837-974f-7353431e3c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = {'question': 3.0}\n",
    "\n",
    "score = np.zeros(len(df))\n",
    "\n",
    "for f in fields:\n",
    "    b = boost.get(f, 1.0)\n",
    "    q = transformers[f].transform([query])\n",
    "    s = cosine_similarity(matrices[f], q).flatten()\n",
    "    score = score + b * s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c27bf-1d0a-4d4d-b26b-ff4f2c7e559e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76926c4a-60c0-4547-a3e9-3ed5c01cc9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = {\n",
    "    'course': 'data-engineering-zoomcamp'\n",
    "}\n",
    "\n",
    "for field, value in filters.items():\n",
    "    mask = (df[field] == value).values\n",
    "    score = score * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da7a34-8052-48a1-9041-bb0f94416448",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.argsort(-score)[:10]\n",
    "results = df.iloc[idx]\n",
    "results.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe39c0",
   "metadata": {},
   "source": [
    "Everything in one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ab72b1-47fd-451b-9660-e4df629c4b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSearch:\n",
    "\n",
    "    def __init__(self, text_fields):\n",
    "        self.text_fields = text_fields\n",
    "        self.matrices = {}\n",
    "        self.vectorizers = {}\n",
    "\n",
    "    def fit(self, records, vectorizer_params={}):\n",
    "        self.df = pd.DataFrame(records)\n",
    "\n",
    "        for f in self.text_fields:\n",
    "            cv = TfidfVectorizer(**vectorizer_params)\n",
    "            X = cv.fit_transform(self.df[f])\n",
    "            self.matrices[f] = X\n",
    "            self.vectorizers[f] = cv\n",
    "\n",
    "    def search(self, query, n_results=10, boost={}, filters={}):\n",
    "        score = np.zeros(len(self.df))\n",
    "\n",
    "        for f in self.text_fields:\n",
    "            b = boost.get(f, 1.0)\n",
    "            q = self.vectorizers[f].transform([query])\n",
    "            s = cosine_similarity(self.matrices[f], q).flatten()\n",
    "            score = score + b * s\n",
    "\n",
    "        for field, value in filters.items():\n",
    "            mask = (self.df[field] == value).values\n",
    "            score = score * mask\n",
    "\n",
    "        idx = np.argsort(-score)[:n_results]\n",
    "        results = self.df.iloc[idx]\n",
    "        return results.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5e13d-02aa-4f77-862e-70f5b14c8067",
   "metadata": {},
   "outputs": [],
   "source": [
    "fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06ee90-31d1-4842-829a-662a812eed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = TextSearch(text_fields=['section', 'question', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3ec354-9382-4fee-94f4-458a5044f019",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecde6ac7-ce80-4b07-b079-55e822f399f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09895ca8-6a96-4e34-90fe-de6a1b117681",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.search(\n",
    "    query='I just signed up. Is it too late to join the course?',\n",
    "    n_results=5,\n",
    "    boost={'question': 3.0},\n",
    "    filters={'course': 'data-engineering-zoomcamp'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a499a8",
   "metadata": {},
   "source": [
    "Up to this point the search has been done by words coincidence, and the implementation was released to minsearch library, which is not recommended for projects at scale as it keeps matrices on memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf21360",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da26fadb-2e59-451b-86f8-09d5c0547fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7a1601-cdae-4299-b3ad-5fa73964b806",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = matrices['text']\n",
    "cv = transformers['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f16a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23973dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c13291-b493-480e-9e10-256eedb9c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=16)\n",
    "X_emb = svd.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6359d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c3105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b68353-1296-43e4-8501-955010f0fc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2600579f-a557-4dde-bc3e-b24afc526ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just signed up. Is it too late to join the course?'\n",
    "\n",
    "Q = cv.transform([query])\n",
    "Q_emb = svd.transform(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4b9419-b36b-45a8-9152-caa31a4761d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103badbb-881b-46ff-b4ab-302265e190cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X_emb[0], Q_emb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c015d99a-396f-4de6-bd8d-719b30eebbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cosine_similarity(X_emb, Q_emb).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53728975-091a-4f59-9ba8-8b3cb97e8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(-score)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80ea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx]\n",
    "# Here there is no course filter, is just a function to show how the search is done with embbedings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d4f7c-cef3-4c15-8c79-d2041afbf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.loc[idx].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003b4a1b",
   "metadata": {},
   "source": [
    "### ðŸ§® Singular Value Decomposition (SVD)\n",
    "\n",
    "**Singular Value Decomposition (SVD)** is a linear algebra technique that factorizes a matrix `A` into three components:\n",
    "\n",
    "```\n",
    "\n",
    "A = U Î£ Váµ—\n",
    "\n",
    "```\n",
    "\n",
    "- `U`: matrix of left singular vectors (documents)\n",
    "- `Î£`: diagonal matrix of singular values (importance of concepts)\n",
    "- `Váµ—`: matrix of right singular vectors (terms)\n",
    "\n",
    "SVD is used to **reduce dimensionality** and uncover **latent semantic structures** in high-dimensional data, especially in **text analysis**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“š Application in Information Retrieval\n",
    "\n",
    "In IR, SVD is the foundation of **Latent Semantic Indexing (LSI)**, where it helps:\n",
    "\n",
    "- Identify concepts behind terms\n",
    "- Capture synonymy and polysemy\n",
    "- Improve retrieval results via concept-based similarity\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“„ Historical Origin\n",
    "\n",
    "SVD itself dates back to early linear algebra work, but its application to IR was popularized by:\n",
    "\n",
    "> **â€œIndexing by Latent Semantic Analysisâ€**  \n",
    "> *Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, Richard Harshman*  \n",
    "> Journal of the American Society for Information Science, 1990.\n",
    "\n",
    "This seminal paper introduced **Latent Semantic Indexing (LSI)** using SVD for document retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§  Note\n",
    "\n",
    "Although SVD/LSI was widely used, it's been largely superseded in practice by **neural embeddings** (e.g., Word2Vec, BERT). However, SVD remains a fundamental technique for **understanding dimensionality reduction** and **concept modeling** in text.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12b1f88-7bb4-4906-a8d0-3ff3bd5f5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab8528-e6e3-4191-9425-fbae117c518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=16)\n",
    "X_emb = nmf.fit_transform(X)\n",
    "X_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73571ba-6f8a-4b24-bd97-18ca6eac92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = cv.transform([query])\n",
    "Q_emb = nmf.transform(Q)\n",
    "Q_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b4efea-db0d-4df5-8408-717ce953ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cosine_similarity(X_emb, Q_emb).flatten()\n",
    "idx = np.argsort(-score)[:10]\n",
    "list(df.loc[idx].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf26468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82705b2d",
   "metadata": {},
   "source": [
    "### ðŸ”¢ Non-Negative Matrix Factorization (NMF)\n",
    "\n",
    "**Non-Negative Matrix Factorization (NMF)** is a dimensionality reduction technique that factorizes a non-negative matrix `A` into two lower-rank non-negative matrices:\n",
    "\n",
    "```\n",
    "\n",
    "A â‰ˆ W Ã— H\n",
    "\n",
    "```\n",
    "\n",
    "- `W`: document-topic matrix\n",
    "- `H`: topic-term matrix\n",
    "\n",
    "Unlike SVD, NMF enforces **non-negativity**, leading to **additive and parts-based representations**. This makes the factors more interpretable, especially in **topic modeling** and **text mining**.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“š Use in Information Retrieval and NLP\n",
    "\n",
    "In text analysis, NMF is commonly applied to **TF-IDF matrices** to extract **latent topics** from documents. Each topic is a weighted combination of terms, and each document is represented as a mixture of topics.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ“„ Historical Origin\n",
    "\n",
    "NMF was introduced in its modern form by:\n",
    "\n",
    "> **â€œLearning the Parts of Objects by Non-Negative Matrix Factorizationâ€**  \n",
    "> *Daniel D. Lee and H. Sebastian Seung*  \n",
    "> *Nature*, 1999\n",
    "\n",
    "Their work demonstrated how NMF could learn meaningful, part-based features from image and text data.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ§  Note\n",
    "\n",
    "NMF remains valuable for interpretable topic modeling. However, it has largely been supplanted by **probabilistic models** (like LDA) and **deep learning approaches** (like BERTopic or neural topic models) that capture richer semantic structures.\n",
    "\n",
    "It's still a great tool when:\n",
    "- You need **interpretable, non-probabilistic topics**\n",
    "- You're working with **TF-IDF** representations\n",
    "- You want a simple and fast alternative to LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edcd82",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0793a42e",
   "metadata": {},
   "source": [
    "The problem with the previous two approaches is that they don't take into account the word order. They just treat all the words separately (that's why it's called \"Bag-of-Words\")\n",
    "\n",
    "BERT and other transformer models don't have this problem.\n",
    "\n",
    "Let's create embeddings with BERT. We will use the Hugging Face library for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03778f26-e16a-4879-897e-7e15797fc364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.eval()  # Set the model to evaluation mode if not training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f95191-3b91-4137-ad6c-d6d22c03afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Yes, we will keep all the materials after the course finishes.\",\n",
    "    \"You can follow the course at your own pace after it finishes\"\n",
    "]\n",
    "encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c51bc5b-3908-42dc-88cc-c07b33a6ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cbdc75-0916-4360-b1a7-c8d03142398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model(**encoded_input)\n",
    "    hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b055bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c208b-d1a6-48d2-9080-130dd2f37f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc911c-5ed7-4a97-86bd-b4bc2e311214",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = hidden_states.mean(dim=1)\n",
    "sentence_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbe88f8-d39f-4b22-8f17-4d911c14e185",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings.numpy()\n",
    "\n",
    "# note that if use a GPU, first you need to move your tensors to CPU\n",
    "# sentence_embeddings_cpu = sentence_embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032e6145-bf8d-4ccf-a592-0bf82c5822f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(seq, n):\n",
    "    result = []\n",
    "    for i in range(0, len(seq), n):\n",
    "        batch = seq[i:i+n]\n",
    "        result.append(batch)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d013583-f000-4c1c-88e4-b05dc5f91504",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57853a2c-2d45-460d-b929-6f6b564ed55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(texts, batch_size=8):\n",
    "    text_batches = make_batches(texts, 8)\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    for batch in tqdm(text_batches):\n",
    "        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_input)\n",
    "            hidden_states = outputs.last_hidden_state\n",
    "            \n",
    "            batch_embeddings = hidden_states.mean(dim=1)\n",
    "            batch_embeddings_np = batch_embeddings.cpu().numpy()\n",
    "            all_embeddings.append(batch_embeddings_np)\n",
    "    \n",
    "    final_embeddings = np.vstack(all_embeddings)\n",
    "    return final_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba794951-7869-407c-a790-ae75419ca294",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20357dc1-3fd1-4b0e-a476-88ce433e347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields = ['section', 'question', 'text']\n",
    "\n",
    "for f in fields:\n",
    "    print(f'computing embeddings for {f}...')\n",
    "    embeddings[f] = compute_embeddings(df[f].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cf0c0-0522-411c-8e6f-3368aafd84e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3fbe18-b993-4d03-890f-0a9f37ad11d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings.bin', 'wb') as f_out:\n",
    "    pickle.dump(embeddings, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe22344-e0f9-40c4-8d51-9e88c79417af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
